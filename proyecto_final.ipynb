{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Final de la materia Minería de Datos I\n",
    "\n",
    "## Olimpia Saucedo Estrada y Francisco Ramírez Castañeda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de Datos\n",
    "Debido a que el conjunto de muestras contiene un mayor número de muestras maliciosas es necesario realizar un balanceo de estas mismas, por lo que se obtuvieron muestras maliciosas aleatoriamente del dataset ajustando el numero de muestras a 400 para posteriormente concatenar las muestras benignas. Con esto se obtuvo un conjunto de muestras balanceado.\n",
    "\n",
    "Con el conjunto de muestras balanceado se realizó el preprocesamiento de los datos mediante los algoritmos TF-IDF y Word2Vec, los cuales permiten la extración de características para poder realizar el entrenamiento de los modelos de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "import features\n",
    "from sklearn.linear_model import Perceptron\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/olibits/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')\n",
    "df_minoria = dataset[dataset.y == 1]\n",
    "df_mayoria = dataset[dataset.y == 0]\n",
    "df_minoria_resample = resample(df_minoria, replace=True, n_samples=400, random_state=123)\n",
    "dataset_balanceado = pd.concat([df_mayoria, df_minoria_resample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Caracterizar las muestras utilizando TF-IDF \n",
    "X,Y = dataset_balanceado['X'],dataset_balanceado['y']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "vec = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Caracterizar las muestras utilizando Word2Vec\n",
    "model_gn = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
    "w2v = dict(zip(model_gn.index2word, model_gn.vectors))\n",
    "X_tokens = [word_tokenize(s) for s in X]\n",
    "X_trainw2v, X_testw2v, Y_trainw2v, Y_testw2v = train_test_split(X_tokens, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9361702127659575\n",
      "0.9361702127659575\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron_vec = Pipeline([('vectorizer', vec), ('pac', perceptron)])\n",
    "\n",
    "perceptron_vec.fit(X_train, Y_train)\n",
    "perceptron_score_vec = perceptron_vec.score(X_test, Y_test)\n",
    "print(perceptron_score_vec)\n",
    "\n",
    "perceptron_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", features.MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"pac\", perceptron)])\n",
    "perceptron_vec.fit(X_train, Y_train)\n",
    "perceptron_score_vec = perceptron_vec.score(X_test, Y_test)\n",
    "print(perceptron_score_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8617021276595744\n",
      "0.9042553191489362\n"
     ]
    }
   ],
   "source": [
    "#TODO 3 Entrenar los algoritmos (Hay que dividirnos esta parte)\n",
    "#maquina de soporte vectorial tfidf\n",
    "svc = svm.SVC(C=10)\n",
    "svm_clf = Pipeline([('vectorizer', vec), ('pac', svc)])\n",
    "svm_clf.fit(X_train, Y_train)\n",
    "svc_score = svm_clf.score(X_test, Y_test)\n",
    "print(svc_score)\n",
    "\n",
    "#maquina de soporte vectorial w2v\n",
    "svm_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", features.MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"pac\", svc)])\n",
    "\n",
    "svm_w2v.fit(X_trainw2v, Y_trainw2v)\n",
    "svc_w2v_score=svm_w2v.score(X_testw2v, Y_testw2v)\n",
    "print(svc_w2v_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9893617021276596\n",
      "0.9680851063829787\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth = 100)\n",
    "\n",
    "forest_sgdc = Pipeline([('vectorizer', vec), ('pac', forest)])\n",
    "forest_sgdc.fit(X_train,Y_train)\n",
    "forest_sgdc_score= forest_sgdc.score(X_test, Y_test)\n",
    "print(forest_sgdc_score)\n",
    "\n",
    "forest_sgdc_w2v = Pipeline([('vectorizer', features.MeanEmbeddingVectorizer(w2v)), ('pac', forest)])\n",
    "forest_sgdc_w2v.fit(X_trainw2v, Y_trainw2v)\n",
    "forest_sgdc_w2v_score=forest_sgdc_w2v.score(X_testw2v, Y_testw2v)\n",
    "print(forest_sgdc_w2v_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9468085106382979\n",
      "0.9598930481283422\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "knn_vec = Pipeline([('vectorizer', vec), ('pac', knn)])\n",
    "knn_vec.fit(X_train, Y_train)\n",
    "knn_vec_score= knn_vec.score(X_test, Y_test)\n",
    "print(knn_vec_score)\n",
    "\n",
    "knn_w2v = Pipeline([('vectorizer', features.MeanEmbeddingVectorizer(w2v)), ('pac', knn)])\n",
    "knn_w2v.fit(X_trainw2v, Y_trainw2v)\n",
    "knn_w2v_score= knn_w2v.score(X_trainw2v, Y_trainw2v)\n",
    "print(knn_w2v_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0dae2d33a0df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Regresion Logistica, caracterizado con TF-IDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlr_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vectorizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'pac'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlr_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "#Regresion Logistica, caracterizado con TF-IDF\n",
    "lr = LogisticRegression(C=1000.0, random_state=0)\n",
    "lr_clf = Pipeline([('vectorizer', vec), ('pac', lr)])\n",
    "lr_clf.fit(X_train, Y_train)\n",
    "lr_score = lr_clf.score(X_test,Y_test)\n",
    "print(lr_score)\n",
    "\n",
    "#Regresion Logistica, caracterizado con W2V\n",
    "vecW2V = features.MeanEmbeddingVectorizer(w2v)\n",
    "lr_w2v_clf = Pipeline([\n",
    "    (\"vectorizer\", vecW2V),\n",
    "    (\"pac\", lr)])\n",
    "\n",
    "lr_w2v_clf.fit(X_trainw2v, Y_trainw2v)\n",
    "lr_w2v_score=lr_w2v_clf.score(X_testw2v, Y_testw2v)\n",
    "print(lr_w2v_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Árboles de decisión TF-IDF\n",
    "dtc =  DecisionTreeClassifier(random_state=0)\n",
    "dt_clf = Pipeline([('vectorizer', vec), ('pac', dtc)])\n",
    "dt_clf.fit(X_train, Y_train)\n",
    "dt_score = dt_clf.score(X_test, Y_test)\n",
    "print(dt_score)\n",
    "#Árboles de decisión w2v\n",
    "dt_clf_w2v = Pipeline([('vectorizer', vecW2V), ('pac', dtc)])\n",
    "dt_clf_w2v.fit(X_trainw2v, Y_trainw2v)\n",
    "dt_clf_score = dt_clf_w2v.score(X_testw2v, Y_testw2v)\n",
    "print(dt_clf_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 4 Comparar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 5 Graficar la matriz de confusión y curva ROC del mejor algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
