{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Final de la materia Minería de Datos I\n",
    "\n",
    "## Olimpia Saucedo Estrada y Francisco Ramírez Castañeda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de Datos\n",
    "Debido a que el conjunto de muestras contiene un mayor número de muestras maliciosas es necesario realizar un balanceo de estas mismas, por lo que se obtuvieron muestras maliciosas aleatoriamente del dataset ajustando el numero de muestras a 400 para posteriormente concatenar las muestras benignas. Con esto se obtuvo un conjunto de muestras balanceado.\n",
    "\n",
    "Con el conjunto de muestras balanceado se realizó el preprocesamiento de los datos mediante los algoritmos TF-IDF y Word2Vec, los cuales permiten la extración de características para poder realizar el entrenamiento de los modelos de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "import features\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/francisco/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')\n",
    "df_minoria = dataset[dataset.y == 1]\n",
    "df_mayoria = dataset[dataset.y == 0]\n",
    "df_minoria_resample = resample(df_minoria, replace=True, n_samples=400, random_state=123)\n",
    "dataset_balanceado = pd.concat([df_mayoria, df_minoria_resample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Caracterizar las muestras utilizando TF-IDF \n",
    "X,Y = dataset_balanceado['X'],dataset_balanceado['y']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "vec = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Caracterizar las muestras utilizando Word2Vec\n",
    "model_gn = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
    "w2v = dict(zip(model_gn.index2word, model_gn.vectors))\n",
    "X_tokens = [word_tokenize(s) for s in X]\n",
    "X_trainw2v, X_testw2v, Y_trainw2v, Y_testw2v = train_test_split(X_tokens, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-91b48d66697d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m perceptron_w2v = Pipeline([\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"word2vec vectorizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanEmbeddingVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     (\"svm\", svc)])\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mperceptron_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mperceptron_score_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperceptron_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svc' is not defined"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron_vec = Pipeline([('vectorizer', vec), ('pac', perceptron)])\n",
    "\n",
    "perceptron_vec.fit(X_train, Y_train)\n",
    "perceptron_score_vec = perceptron_vec.score(X_test, Y_test)\n",
    "perceptron_score_vec\n",
    "\n",
    "perceptron_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", features.MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"svm\", svc)])\n",
    "perceptron_vec.fit(X_train, Y_train)\n",
    "perceptron_score_vec = perceptron_vec.score(X_test, Y_test)\n",
    "perceptron_score_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8617021276595744\n",
      "0.9042553191489362\n"
     ]
    }
   ],
   "source": [
    "#TODO 3 Entrenar los algoritmos (Hay que dividirnos esta parte)\n",
    "#maquina de soporte vectorial tfidf\n",
    "svc = svm.SVC(C=10)\n",
    "svm_clf = Pipeline([('vectorizer', vec), ('pac', svc)])\n",
    "svm_clf.fit(X_train, Y_train)\n",
    "svc_score = svm_clf.score(X_test, Y_test)\n",
    "print(svc_score)\n",
    "\n",
    "#maquina de soporte vectorial w2v\n",
    "svm_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", features.MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"svm\", svc)])\n",
    "\n",
    "svm_w2v.fit(X_trainw2v, Y_trainw2v)\n",
    "svc_w2v_score=svm_w2v.score(X_testw2v, Y_testw2v)\n",
    "print(svc_w2v_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Pipeline in module sklearn.pipeline:\n",
      "\n",
      "class Pipeline(sklearn.utils.metaestimators._BaseComposition)\n",
      " |  Pipeline of transforms with a final estimator.\n",
      " |  \n",
      " |  Sequentially apply a list of transforms and a final estimator.\n",
      " |  Intermediate steps of the pipeline must be 'transforms', that is, they\n",
      " |  must implement fit and transform methods.\n",
      " |  The final estimator only needs to implement fit.\n",
      " |  The transformers in the pipeline can be cached using ``memory`` argument.\n",
      " |  \n",
      " |  The purpose of the pipeline is to assemble several steps that can be\n",
      " |  cross-validated together while setting different parameters.\n",
      " |  For this, it enables setting parameters of the various steps using their\n",
      " |  names and the parameter name separated by a '__', as in the example below.\n",
      " |  A step's estimator may be replaced entirely by setting the parameter\n",
      " |  with its name to another estimator, or a transformer removed by setting\n",
      " |  to None.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <pipeline>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  steps : list\n",
      " |      List of (name, transform) tuples (implementing fit/transform) that are\n",
      " |      chained, in the order in which they are chained, with the last object\n",
      " |      an estimator.\n",
      " |  \n",
      " |  memory : None, str or object with the joblib.Memory interface, optional\n",
      " |      Used to cache the fitted transformers of the pipeline. By default,\n",
      " |      no caching is performed. If a string is given, it is the path to\n",
      " |      the caching directory. Enabling caching triggers a clone of\n",
      " |      the transformers before fitting. Therefore, the transformer\n",
      " |      instance given to the pipeline cannot be inspected\n",
      " |      directly. Use the attribute ``named_steps`` or ``steps`` to\n",
      " |      inspect estimators within the pipeline. Caching the\n",
      " |      transformers is advantageous when fitting is time consuming.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  named_steps : bunch object, a dictionary with attribute access\n",
      " |      Read-only attribute to access any step parameter by user given name.\n",
      " |      Keys are step names and values are steps parameters.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import svm\n",
      " |  >>> from sklearn.datasets import samples_generator\n",
      " |  >>> from sklearn.feature_selection import SelectKBest\n",
      " |  >>> from sklearn.feature_selection import f_regression\n",
      " |  >>> from sklearn.pipeline import Pipeline\n",
      " |  >>> # generate some data to play with\n",
      " |  >>> X, y = samples_generator.make_classification(\n",
      " |  ...     n_informative=5, n_redundant=0, random_state=42)\n",
      " |  >>> # ANOVA SVM-C\n",
      " |  >>> anova_filter = SelectKBest(f_regression, k=5)\n",
      " |  >>> clf = svm.SVC(kernel='linear')\n",
      " |  >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\n",
      " |  >>> # You can set the parameters using the names issued\n",
      " |  >>> # For instance, fit using a k of 10 in the SelectKBest\n",
      " |  >>> # and a parameter 'C' of the svm\n",
      " |  >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\n",
      " |  ...                      # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      " |  Pipeline(memory=None,\n",
      " |           steps=[('anova', SelectKBest(...)),\n",
      " |                  ('svc', SVC(...))])\n",
      " |  >>> prediction = anova_svm.predict(X)\n",
      " |  >>> anova_svm.score(X, y)                        # doctest: +ELLIPSIS\n",
      " |  0.829...\n",
      " |  >>> # getting the selected features chosen by anova_filter\n",
      " |  >>> anova_svm.named_steps['anova'].get_support()\n",
      " |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      " |  array([False, False,  True,  True, False, False, True,  True, False,\n",
      " |         True,  False,  True,  True, False, True,  False, True, True,\n",
      " |         False, False], dtype=bool)\n",
      " |  >>> # Another way to get selected features chosen by anova_filter\n",
      " |  >>> anova_svm.named_steps.anova.get_support()\n",
      " |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      " |  array([False, False,  True,  True, False, False, True,  True, False,\n",
      " |         True,  False,  True,  True, False, True,  False, True, True,\n",
      " |         False, False], dtype=bool)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Pipeline\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, steps, memory=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Apply transforms, and decision_function of the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_score : array-like, shape = [n_samples, n_classes]\n",
      " |  \n",
      " |  fit(self, X, y=None, **fit_params)\n",
      " |      Fit the model\n",
      " |      \n",
      " |      Fit all the transforms one after the other and transform the\n",
      " |      data, then fit the transformed data using the final estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Training data. Must fulfill input requirements of first step of the\n",
      " |          pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Training targets. Must fulfill label requirements for all steps of\n",
      " |          the pipeline.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of each step, where\n",
      " |          each parameter name is prefixed such that parameter ``p`` for step\n",
      " |          ``s`` has key ``s__p``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : Pipeline\n",
      " |          This estimator\n",
      " |  \n",
      " |  fit_predict(self, X, y=None, **fit_params)\n",
      " |      Applies fit_predict of last step in pipeline after transforms.\n",
      " |      \n",
      " |      Applies fit_transforms of a pipeline to the data, followed by the\n",
      " |      fit_predict method of the final estimator in the pipeline. Valid\n",
      " |      only if the final estimator implements fit_predict.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Training data. Must fulfill input requirements of first step of\n",
      " |          the pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Training targets. Must fulfill label requirements for all steps\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of each step, where\n",
      " |          each parameter name is prefixed such that parameter ``p`` for step\n",
      " |          ``s`` has key ``s__p``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : array-like\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit the model and transform with the final estimator\n",
      " |      \n",
      " |      Fits all the transforms one after the other and transforms the\n",
      " |      data, then uses fit_transform on transformed data with the final\n",
      " |      estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Training data. Must fulfill input requirements of first step of the\n",
      " |          pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Training targets. Must fulfill label requirements for all steps of\n",
      " |          the pipeline.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of each step, where\n",
      " |          each parameter name is prefixed such that parameter ``p`` for step\n",
      " |          ``s`` has key ``s__p``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape = [n_samples, n_transformed_features]\n",
      " |          Transformed samples\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Apply transforms to the data, and predict with the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : array-like\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Apply transforms, and predict_log_proba of the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_score : array-like, shape = [n_samples, n_classes]\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Apply transforms, and predict_proba of the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_proba : array-like, shape = [n_samples, n_classes]\n",
      " |  \n",
      " |  score(self, X, y=None, sample_weight=None)\n",
      " |      Apply transforms, and score with the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Targets used for scoring. Must fulfill label requirements for all\n",
      " |          steps of the pipeline.\n",
      " |      \n",
      " |      sample_weight : array-like, default=None\n",
      " |          If not None, this argument is passed as ``sample_weight`` keyword\n",
      " |          argument to the ``score`` method of the final estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  set_params(self, **kwargs)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      Valid parameter keys can be listed with ``get_params()``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  inverse_transform\n",
      " |      Apply inverse transformations in reverse order\n",
      " |      \n",
      " |      All estimators in the pipeline must support ``inverse_transform``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : array-like, shape = [n_samples, n_transformed_features]\n",
      " |          Data samples, where ``n_samples`` is the number of samples and\n",
      " |          ``n_features`` is the number of features. Must fulfill\n",
      " |          input requirements of last step of pipeline's\n",
      " |          ``inverse_transform`` method.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape = [n_samples, n_features]\n",
      " |  \n",
      " |  named_steps\n",
      " |  \n",
      " |  transform\n",
      " |      Apply transforms, and transform with the final estimator\n",
      " |      \n",
      " |      This also works where final estimator is ``None``: all prior\n",
      " |      transformations are applied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to transform. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape = [n_samples, n_transformed_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9574468085106383\n",
      "0.9893617021276596\n"
     ]
    }
   ],
   "source": [
    "#Regresion Logistica, caracterizado con TF-IDF\n",
    "lr = LogisticRegression(C=1000.0, random_state=0)\n",
    "lr_clf = Pipeline([('vectorizer', vec), ('pac', lr)])\n",
    "lr_clf.fit(X_train, Y_train)\n",
    "lr_score = lr_clf.score(X_test,Y_test)\n",
    "print(lr_score)\n",
    "\n",
    "#Regresion Logistica, caracterizado con W2V\n",
    "vecW2V = features.MeanEmbeddingVectorizer(w2v)\n",
    "lr_w2v_clf = Pipeline([\n",
    "    (\"vectorizer\", vecW2V),\n",
    "    (\"pac\", lr)])\n",
    "\n",
    "lr_w2v_clf.fit(X_trainw2v, Y_trainw2v)\n",
    "lr_w2v_score=lr_w2v_clf.score(X_testw2v, Y_testw2v)\n",
    "print(lr_w2v_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9680851063829787\n",
      "0.9361702127659575\n"
     ]
    }
   ],
   "source": [
    "#Árboles de decisión TF-IDF\n",
    "dtc =  DecisionTreeClassifier(random_state=0)\n",
    "dt_clf = Pipeline([('vectorizer', vec), ('pac', dtc)])\n",
    "dt_clf.fit(X_train, Y_train)\n",
    "dt_score = dt_clf.score(X_test, Y_test)\n",
    "print(dt_score)\n",
    "#Árboles de decisión w2v\n",
    "dt_clf_w2v = Pipeline([('vectorizer', vecW2V), ('pac', dtc)])\n",
    "dt_clf_w2v.fit(X_trainw2v, Y_trainw2v)\n",
    "dt_clf_score = dt_clf_w2v.score(X_testw2v, Y_testw2v)\n",
    "print(dt_clf_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 4 Comparar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 5 Graficar la matriz de confusión y curva ROC del mejor algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
